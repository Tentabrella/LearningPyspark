{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create from RDD\n",
    "some_rdd = sc.parallelize([Row(name = 'John', age = 19),\n",
    "                          Row(name = 'Bob', age = 21)])\n",
    "some_df = sqlContext.createDataFrame(some_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema\n",
    "schema = StructType([StructField('personal_name', StringType(), False),\n",
    "                    StructField('age', IntegerType(), False)])\n",
    "another_df = sqlContext.createDataFrame(some_rdd, schema)\n",
    "# another_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from other type of files\n",
    "# read from pandas\n",
    "import pandas as pd\n",
    "pd_df = sqlContext.createDataFrame(pd.DataFrame(range(5)))\n",
    "# or \n",
    "pd_df = spark.createDataFrame(pd.DataFrame(range(5)))\n",
    "# pd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from json\n",
    "import json\n",
    "data = [ { 'a' : 1, 'b' : 2, 'c' : 3, 'd' : 4, 'e' : 5 } ]\n",
    "with open('test_data.json', 'w') as add:\n",
    "    json.dump(data, add)\n",
    "json_df = sqlContext.read.json('test_data.json')\n",
    "# or \n",
    "json_df = spark.read.json('test_data.json')\n",
    "# json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv\n",
    "csv_df = spark.read.csv('titanic.csv', header = True)\n",
    "# csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet\n",
    "parquet_df = spark.read.load('part-00000-82db5734-5b30-4b12-8c75-3d0197e2f1b0-c000.snappy.parquet')\n",
    "# parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------+---+-----+-----+-----+-----+-----+-----+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+--------+------+------+--------+\n",
      "|Passengerid|Age|   Fare|Sex|sibsp|zero5|zero6|zero7|zero8|zero9|zero10|zero11|Parch|zero13|zero14|zero15|zero16|zero17|zero18|zero19|zero20|Pclass|zero22|zero23|Embarked|zero25|zero26|2urvived|\n",
      "+-----------+---+-------+---+-----+-----+-----+-----+-----+-----+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+--------+------+------+--------+\n",
      "|          1| 22|   7.25|  0|    1|    0|    0|    0|    0|    0|     0|     0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     3|     0|     0|       2|     0|     0|       0|\n",
      "|          2| 38|71.2833|  1|    1|    0|    0|    0|    0|    0|     0|     0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     1|     0|     0|       0|     0|     0|       1|\n",
      "|          3| 26|  7.925|  1|    0|    0|    0|    0|    0|    0|     0|     0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     3|     0|     0|       2|     0|     0|       1|\n",
      "|          4| 35|   53.1|  1|    1|    0|    0|    0|    0|    0|     0|     0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     1|     0|     0|       2|     0|     0|       1|\n",
      "|          5| 35|   8.05|  0|    0|    0|    0|    0|    0|    0|     0|     0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     3|     0|     0|       2|     0|     0|       0|\n",
      "+-----------+---+-------+---+-----+-----+-----+-----+-----+-----+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+--------+------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check some row\n",
    "csv_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Passengerid: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- sibsp: string (nullable = true)\n",
      " |-- zero5: string (nullable = true)\n",
      " |-- zero6: string (nullable = true)\n",
      " |-- zero7: string (nullable = true)\n",
      " |-- zero8: string (nullable = true)\n",
      " |-- zero9: string (nullable = true)\n",
      " |-- zero10: string (nullable = true)\n",
      " |-- zero11: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- zero13: string (nullable = true)\n",
      " |-- zero14: string (nullable = true)\n",
      " |-- zero15: string (nullable = true)\n",
      " |-- zero16: string (nullable = true)\n",
      " |-- zero17: string (nullable = true)\n",
      " |-- zero18: string (nullable = true)\n",
      " |-- zero19: string (nullable = true)\n",
      " |-- zero20: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- zero22: string (nullable = true)\n",
      " |-- zero23: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- zero25: string (nullable = true)\n",
      " |-- zero26: string (nullable = true)\n",
      " |-- 2urvived: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take\n",
    "aaa = csv_df.take(5)\n",
    "type(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to rdd\n",
    "df_rdd = csv_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "csv_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+---+-----+-----+-----+-----+-----+-----+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+--------+------+------+--------+\n",
      "|Passengerid|Age|Fare|Sex|sibsp|zero5|zero6|zero7|zero8|zero9|zero10|zero11|Parch|zero13|zero14|zero15|zero16|zero17|zero18|zero19|zero20|Pclass|zero22|zero23|Embarked|zero25|zero26|2urvived|\n",
      "+-----------+---+----+---+-----+-----+-----+-----+-----+-----+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+--------+------+------+--------+\n",
      "+-----------+---+----+---+-----+-----+-----+-----+-----+-----+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+--------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# is nul\n",
    "from pyspark.sql.functions import isnull\n",
    "csv_df.filter(isnull(\"Age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
